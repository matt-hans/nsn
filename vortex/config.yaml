# Vortex Configuration
# VRAM budget: Adjust based on GPU (RTX 3060 12GB, RTX 3090 24GB, etc.)

device:
  # Primary compute device - "cuda:0" for GPU, "cpu" for fallback (testing only)
  name: "cuda:0"
  # Enable TF32 for faster matmul on Ampere+ GPUs
  allow_tf32: true

vram:
  # Soft limit - log warning but continue
  # Optimized for RTX 3060 12GB (minimum target hardware)
  soft_limit_gb: 10.5
  # Hard limit - raise error and abort
  # For RTX 3090 testing: 22GB. For RTX 3060: use offloading="always"
  hard_limit_gb: 22.0
  # Monitoring interval (seconds)
  monitor_interval_sec: 5.0

models:
  # Model precision overrides (for debugging or VRAM optimization)
  # Production values: flux=nf4, cogvideox=int8, kokoro=fp32, clip=fp16
  # Note: CLIP INT8 disabled due to open_clip compatibility issues; FP16 saves ~50% VRAM
  precision:
    flux: "nf4"           # 4-bit NormalFloat quantization
    cogvideox: "int8"     # INT8 quantization for VRAM efficiency
    showrunner: "external" # Ollama runs externally (not GPU-resident)
    kokoro: "fp32"        # Full precision (small model, quality matters)
    clip_b: "fp16"        # Half precision (INT8 incompatible with open_clip)
    clip_l: "fp16"        # Half precision (INT8 incompatible with open_clip)

  # Force local-only model loading (no network) and shared cache root.
  # Ensure all required weights are pre-cached in this directory.
  local_only: true
  cache_dir: "/home/matt/.cache/huggingface/hub"

  # Sequential offloading: Move models to CPU RAM between pipeline stages
  # Reduces peak VRAM from ~11.8GB to ~6.5GB (enables GPUs with <12GB)
  # Options: "auto" (enable if VRAM < 12GB), "always" (force), "never" (disable)
  offloading: "always"

# LLM configuration for Showrunner script generation
llm:
  # Ollama configuration for script generation
  provider: "ollama"
  base_url: "http://localhost:11434"
  model: "llama3:8b"
  timeout_s: 30
  # Fallback to templates if Ollama unavailable
  fallback_to_templates: true

buffers:
  # Pre-allocated output buffers (prevents fragmentation during generation)
  # Sizes in pixels/samples - buffers allocated at pipeline initialization
  actor:
    height: 512
    width: 512
    channels: 3
  video:
    # Reduced from 1080 (45s) to 360 (15s) to save ~2.2GB VRAM
    # Most TTS scripts are <15s; longer videos can allocate dynamically
    frames: 360           # 15 seconds * 24 fps (~1.0GB vs 3.2GB)
    height: 512
    width: 512
    channels: 3
  audio:
    sample_rate: 24000
    duration_sec: 15      # Match video buffer duration
    samples: 360000       # 24000 * 15

pipeline:
  # Slot generation timeout (seconds) - abort if exceeded
  # Audio-gated pipeline: ~3s audio + ~12s image + ~25s video + ~2s clip = ~42s
  generation_timeout_sec: 60.0
  # Enable async parallelization for audio + image generation
  parallel_audio_actor: true

quality:
  prompt_steering:
    enabled: true
    # NOTE: Focused on keyframe generation quality for CogVideoX input
    positive_prefix: "medium shot, looking at viewer, symmetrical face, centered composition, eye contact, "
    negative_prefix: "profile view, side view, looking away, skewed, distorted, back of head, asymmetrical"
  camera_shake:
    enabled: true
    strength: 0.002
    zoom: 0.002

outputs:
  # Persist RenderResult to disk after successful generation
  enabled: true
  # Output directory for .mp4/.wav assets
  directory: "outputs"
  # Attempt to mux audio into mp4 (falls back to video-only on failure)
  include_audio_in_mp4: true

# Lane 0 renderer configuration
renderers:
  # Enable renderer system (uses RendererRegistry)
  enabled: true
  # Directory containing renderer folders with manifest.yaml
  directory: "renderers"
  # Default renderer to use (must match a registered renderer name)
  default: "default-narrative-chain"
  # Renderer policy limits
  policy:
    # Maximum VRAM any renderer can declare (GB)
    max_vram_gb: 12.0
    # Maximum latency for Lane 0 (ms) - audio-gated pipeline ~42s typical
    max_latency_ms: 65000
    # Require determinism for Lane 0 (must be true for BFT consensus)
    require_determinism: true
    # Allowlist of renderer names (null = allow all registered renderers)
    allowlist: null

plugins:
  # Enable plugin discovery for custom renderers
  enabled: true
  # Directory containing plugin folders with manifest.yaml
  directory: "plugins"
  # Sandbox execution settings (required for untrusted plugins)
  sandbox:
    # Enable sandboxed execution (Docker by default)
    enabled: false
    # Sandbox engine: "docker" (recommended) or "process" (dev only)
    engine: "docker"
    # Docker image containing Vortex runtime + sandbox runner
    docker_image: "nsn-vortex:latest"
    # Docker CLI binary name/path
    docker_bin: "docker"
    # Network mode for plugin containers (default denies outbound network)
    network: "none"
    # Resource limits (host-side cgroup caps)
    memory_mb: 4096
    cpu_cores: 2.0
    pids_limit: 256
    tmpfs_mb: 64
    # Optional GPU request (e.g., "all" or "device=0")
    gpus: null
    # Grace period added to timeout before killing container
    timeout_grace_ms: 5000
    # Seccomp profile ("default" uses Docker default, or provide a profile path)
    seccomp_profile: "default"
    # Allow unsafe sandbox engines (process) - requires VORTEX_ALLOW_UNSAFE_SANDBOX=1
    allow_insecure: false
  # Plugin policy limits (defaults favor Lane 0 latency protection)
  policy:
    # Maximum VRAM any plugin can declare (GB)
    max_vram_gb: 11.5
    # Lane-specific latency budgets (ms)
    lane0_max_latency_ms: 60000
    lane1_max_latency_ms: 120000
    # SECURITY: Untrusted plugins are DISABLED by default.
    # Only explicitly allowlisted plugins can execute.
    # Set to true ONLY for development/testing with trusted plugin sources.
    # WARNING: Enabling untrusted plugins allows arbitrary Python code execution!
    allow_untrusted: false
    # Allowlist of plugin names that may execute even with allow_untrusted=false
    allowlist: ["vortex-lane0"]

# Slot timing orchestration (T020)
orchestration:
  # Per-stage timeouts (seconds) - prevent infinite hangs
  timeouts:
    audio_s: 5     # Kokoro TTS timeout (2s target)
    image_s: 20    # Flux-Schnell timeout (12s target)
    video_s: 120   # CogVideoX timeout (60-90s typical)
    clip_s: 5      # Dual CLIP ensemble timeout (1s target)

  # Retry policy per stage (0 = no retry)
  retry_policy:
    audio: 1   # Retry audio once (recovers from transient CUDA errors)
    image: 0   # No image retry (too expensive time-wise)
    video: 0   # No video retry (too expensive time-wise)
    clip: 0    # No CLIP retry (fast, rarely fails)

  # Deadline buffer (seconds) - safety margin for deadline tracking
  # Larger buffer = more conservative abort (fewer false positives)
  # Smaller buffer = more aggressive (risk missing deadline)
  deadline_buffer_s: 5

logging:
  # JSON structured logging for Vector â†’ Loki aggregation
  format: "json"
  level: "INFO"
  # Include VRAM stats in every log message
  include_vram_stats: true
