# CLIP Model Configuration
# Uses FP16 precision for VRAM efficiency (INT8 disabled due to open_clip compatibility)
# Total VRAM: ~0.6 GB (FP16) vs ~1.2 GB (FP32)

models:
  clip_vit_b_32:
    architecture: "ViT-B-32"
    pretrained: "openai"
    # INT8 disabled - causes compatibility issues with open_clip's transformer layers
    # (get_weight_dtype fails on quantized Linear). Using FP16 instead.
    precision: "fp16"
    vram_budget_gb: 0.2
    cache_path: "~/.cache/vortex/clip"

  clip_vit_l_14:
    architecture: "ViT-L-14"
    pretrained: "openai"
    precision: "fp16"
    vram_budget_gb: 0.4
    cache_path: "~/.cache/vortex/clip"

ensemble:
  weight_b32: 0.4  # Weight for ViT-B-32
  weight_l14: 0.6  # Weight for ViT-L-14

  self_check_thresholds:
    score_b: 0.70  # Minimum ViT-B-32 score for self-check pass
    score_l: 0.72  # Minimum ViT-L-14 score for self-check pass

  outlier_detection:
    divergence_threshold: 0.15  # Flag if |score_b - score_l| > this value

  keyframe_sampling:
    num_frames: 5  # Number of keyframes to sample from video
    method: "evenly_spaced"  # Sampling strategy

performance:
  target_latency_p99_ms: 1000  # <1s P99 for 5-frame verification
  target_vram_total_gb: 0.6    # Combined VRAM budget (FP16)

notes: |
  FP16 precision reduces VRAM by ~50% vs FP32 with negligible accuracy loss.
  INT8 quantization disabled due to open_clip compatibility issues.
  Dual ensemble reduces false positives/negatives by ~40% vs single model.
  Self-check thresholds prevent wasted BFT rounds on low-quality content.
