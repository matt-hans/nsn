# LivePortrait FP16 Configuration
# Model configuration for audio-driven video warping with 3.5GB VRAM budget

model:
  name: "liveportrait-base-fp16"
  source: "local"  # local-only; set runtime.repo_path to the LivePortrait repo
  repo_id: "KwaiVGI/LivePortrait"

  # Precision settings
  precision: "fp16"  # torch.float16
  compute_dtype: "float16"

  # VRAM budget
  vram_budget_gb: 3.5
  vram_min_gb: 3.0
  vram_max_gb: 4.0

# Performance optimization
optimization:
  use_tensorrt: true  # Enable TensorRT if available (20-30% speedup)
  tensorrt_engine_path: "~/.cache/vortex/liveportrait_trt.engine"
  batch_size: 1  # Sequential frame generation (batching uses too much VRAM)
  num_workers: 0  # No multiprocessing (GPU-bound)

# Video output settings
output:
  fps: 24  # Cinema standard frame rate
  resolution: [512, 512]  # Height × Width (matches Flux actor input)
  max_duration_sec: 45  # Maximum video duration per slot
  format: "TCHW"  # Tensor format: Time × Channels × Height × Width

# Audio settings
audio:
  sample_rate: 24000  # 24kHz mono (matches Kokoro output)
  frame_duration_ms: 41.67  # ~42ms per frame at 24fps (1000ms / 24fps)
  lookahead_frames: 2  # Lookahead for smoother lip-sync

# Expression presets
expressions:
  neutral:
    intensity: 0.3
    eye_openness: 0.5
    mouth_scale: 1.0
    head_motion: 0.2
    description: "Minimal expression, calm demeanor"

  excited:
    intensity: 0.8
    eye_openness: 0.8
    mouth_scale: 1.2
    head_motion: 0.6
    description: "Energetic, wide eyes, animated gestures"

  manic:
    intensity: 1.0
    eye_openness: 0.9
    mouth_scale: 1.3
    head_motion: 0.8
    description: "Maximum energy, Rick-like enthusiasm"

  calm:
    intensity: 0.2
    eye_openness: 0.4
    mouth_scale: 0.9
    head_motion: 0.1
    description: "Subdued, low energy, minimal movement"

# Lip-sync settings
lipsync:
  viseme_dim: 3  # [jaw_open, lip_width, lip_rounding]
  phoneme_to_viseme_mapping:
    # Vowels
    - phonemes: ["AA", "AH", "AO"]  # a, ah, aw
      viseme: [0.8, 0.6, 0.3]  # Wide open jaw, neutral lips
    - phonemes: ["EH", "AE"]  # e, a
      viseme: [0.5, 0.7, 0.2]  # Half open jaw, wide lips
    - phonemes: ["IH", "IY"]  # i, ee
      viseme: [0.3, 0.8, 0.1]  # Small jaw, wide lips
    - phonemes: ["UH", "UW"]  # u, oo
      viseme: [0.4, 0.4, 0.8]  # Medium jaw, rounded lips
    - phonemes: ["ER"]  # er
      viseme: [0.4, 0.5, 0.4]  # Medium jaw, neutral

    # Consonants
    - phonemes: ["B", "P", "M"]  # Bilabials
      viseme: [0.1, 0.3, 0.9]  # Closed lips, rounded
    - phonemes: ["F", "V"]  # Labiodentals
      viseme: [0.2, 0.5, 0.3]  # Small opening, lip-teeth contact
    - phonemes: ["TH", "DH"]  # Dentals
      viseme: [0.3, 0.6, 0.2]  # Tongue visible
    - phonemes: ["T", "D", "N", "L"]  # Alveolars
      viseme: [0.3, 0.5, 0.3]  # Neutral
    - phonemes: ["K", "G", "NG"]  # Velars
      viseme: [0.4, 0.5, 0.3]  # Back of mouth
    - phonemes: ["CH", "JH", "SH", "ZH"]  # Affricates/fricatives
      viseme: [0.3, 0.4, 0.4]  # Slight rounding
    - phonemes: ["S", "Z"]  # Sibilants
      viseme: [0.2, 0.6, 0.2]  # Narrow opening
    - phonemes: ["R"]  # Rhotic
      viseme: [0.4, 0.4, 0.5]  # Rounded
    - phonemes: ["W", "Y"]  # Glides
      viseme: [0.3, 0.5, 0.6]  # Slight rounding
    - phonemes: ["H"]  # Glottal
      viseme: [0.4, 0.5, 0.3]  # Neutral open

  # Smoothing parameters - reduced for snappier lip motion
  smoothing_window: 1  # Reduced from 3 for snappier lips
  transition_speed: 0.6  # Slightly faster transitions
  method: "wav2vec"

  # Noise suppression - filters micro-movements to prevent twitching
  noise_threshold: 0.02  # Suppress micro-movements below this
  expression_scale: 1.3  # Amplify speech movements

  # Accuracy target - tighter sync
  alignment_tolerance_frames: 1  # Tighter sync (was 2)

# Expression sequence transitions
transitions:
  interpolation: "cubic"  # "linear", "cubic", "ease-in-out"
  keyframe_duration_sec: 11.25  # 45s / 4 expressions = 11.25s each
  blend_duration_sec: 1.0  # Transition blend time between expressions

# Model weights cache
cache:
  directory: "~/.cache/vortex/liveportrait"
  weights_filename: "pretrained_weights"
  tensorrt_filename: "liveportrait_trt.engine"
  download_source: "local_only"

# Runtime integration settings
runtime:
  backend: "gpu"  # gpu, cli (no fallback - requires real LivePortrait)
  # Set via LIVEPORTRAIT_HOME env var, or override here:
  repo_path: "/home/matt/nsn/LivePortrait"
  # Use d7.pkl which has proper c_d_eyes_lst/c_d_lip_lst keys (talking.pkl lacks these)
  driving_source: "/home/matt/nsn/LivePortrait/assets/examples/driving/d7.pkl"
  output_dirs:
    - "~/.cache/vortex/liveportrait/outputs"
  animation_region: "all"
  crop_driving_video: false
  relative_motion: true
  use_output_dir_flag: false
  driving_multiplier: 1.2
  crop_scale: 2.3
  crop_vx_ratio: 0.4
  crop_vy_ratio: -0.125
  # LivePortrait built-in jitter filter
  driving_smooth_observation_variance: 3e-7

# Post-processing settings for stitching/mask (bearded characters)
post_processing:
  mask_crop_expansion: 1.2  # Expand mask to include chin/beard (1.2+ for procedural jaw)
  stitch_blur_radius: 5  # Subtle blur on stitching seam

# Performance targets
performance:
  target_latency_p99_sec: 8.0  # P99 generation time for 45s video on RTX 3060
  target_throughput_fps: 135  # 1080 frames / 8 seconds = 135 fps generation
  warmup_iterations: 3  # Warmup runs to stabilize CUDA kernels

# Monitoring
monitoring:
  enable_vram_tracking: true
  log_frame_generation_time: false  # Too verbose, disable in production
  alert_on_vram_spike: true
  vram_spike_threshold_gb: 0.5  # Alert if VRAM increases by >500MB
